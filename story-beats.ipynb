{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4750b17-3e17-4bea-ac92-0886ca139561",
   "metadata": {},
   "source": [
    "## Imports and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1639528a-1137-4270-8c2b-b3ef481c427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4526c69-03c4-4a09-a494-a3f00d9c2fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMPS_PATH = 'train/train/scene_timestamps'\n",
    "FEATURES_PATH = 'train/train/features'\n",
    "LABELS_PATH = 'train/train/labels'\n",
    "SUBTITLES_PATH = 'train/train/subtitles'\n",
    "\n",
    "TEST_TIMESTAMPS_PATH = 'test/test/scene_timestamps'\n",
    "TEST_FEATURES_PATH = 'test/test/features'\n",
    "TEST_SUBTITLES_PATH = 'test/test/subtitles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a14a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check GPU available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000a3d5-7031-41ca-8e30-21753aacfbb6",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2e1c0-23af-428c-8112-10f7e0d6806b",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac399c52-72f6-4643-be29-7f922e00447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(path):\n",
    "    if os.path.exists(path):\n",
    "        return os.listdir(path)\n",
    "    else:\n",
    "        print(\"PATH DOES NOT EXIST!\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccb21fde-14d1-48e9-843d-adec49a4c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_name(file):\n",
    "    return file[10:-15]\n",
    "\n",
    "def get_movie_id(file):\n",
    "    return file[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "536fd822-2c92-493b-b3d6-b13fa53a725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_file(file):\n",
    "    return file.replace(\"_timestamps\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6315a7a-d497-4fd4-b13f-0d221a94634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_csv(path, movie_name):\n",
    "    df = pd.read_csv(path)\n",
    "    df.rename(columns={\"Unnamed: 0\": \"scene_id\"}, inplace=True)\n",
    "    df[\"movie\"] = movie_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "487bbef4-02d3-4e56-b7df-d2abacfb907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_movie_info(files):\n",
    "    movie_ids, movies = [], []\n",
    "    for file in files:\n",
    "        movie_name = get_movie_name(file)\n",
    "        movie_id = get_movie_id(file)\n",
    "        movie_ids.append(movie_id)\n",
    "        movies.append(movie_name)\n",
    "    return movie_ids, movies\n",
    "\n",
    "def prepare_dataframes(files, movies, path_func, data_func):\n",
    "    dfs = []\n",
    "    for idx, file in enumerate(files):\n",
    "        path = path_func(file)\n",
    "        df = data_func(path, movies[idx])\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def merge_dataframes(df1, df2):\n",
    "    test_full_data = pd.merge(df1, df2, on=[\"scene_id\", \"movie\"])\n",
    "    test_full_data[\"end\"] = test_full_data[\"start\"] + test_full_data[\"s_dur\"]\n",
    "    return test_full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859a0bf-eda8-445d-8c8a-4d166393b1c9",
   "metadata": {},
   "source": [
    "### Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a2fabdc-1f16-4dfb-9b0d-e761b72e231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = load_files(TIMESTAMPS_PATH)\n",
    "# remove australia because australia subtitles are in diffrent format than everything else\n",
    "train_files.remove(\"tt0455824_australia_timestamps.csv\")\n",
    "\n",
    "test_files = load_files(TEST_TIMESTAMPS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd91cd-a851-4157-9860-850ad3737162",
   "metadata": {},
   "source": [
    "### Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6798481c-e78e-4f88-87ea-4a0fa6451f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_movies_ids, train_movies = extract_movie_info(train_files)\n",
    "test_movies_ids, test_movies = extract_movie_info(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f59cca-5f69-44f2-9246-539cce0e99ae",
   "metadata": {},
   "source": [
    "### Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b51c99ee-7ffc-4df7-9c7c-932e7343bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_timestamps = prepare_dataframes(\n",
    "    train_files,\n",
    "    train_movies,\n",
    "    lambda file: os.path.join(TIMESTAMPS_PATH, file), \n",
    "    prepare_csv\n",
    ")\n",
    "\n",
    "test_timestamps = prepare_dataframes(\n",
    "    test_files,\n",
    "    test_movies,\n",
    "    lambda file: os.path.join(TEST_TIMESTAMPS_PATH, file), \n",
    "    prepare_csv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2befcf-73ce-4bde-8e7f-febf7891d147",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc254cd7-894b-4afa-9f26-c31b3974db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = prepare_dataframes(\n",
    "    train_files,\n",
    "    train_movies,\n",
    "    lambda file: os.path.join(FEATURES_PATH, get_feature_file(file)), \n",
    "    prepare_csv\n",
    ")\n",
    "\n",
    "test_features = prepare_dataframes(\n",
    "    test_files,\n",
    "    test_movies,\n",
    "    lambda file: os.path.join(TEST_FEATURES_PATH, get_feature_file(file)), \n",
    "    prepare_csv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8467b2d-3432-42a9-964c-226ac931c302",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b36a21f1-9d62-4a9f-a51f-c355a49a96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for idx, file in enumerate(train_files):\n",
    "    # print(feature_files[idx])\n",
    "    labels_path = os.path.join(LABELS_PATH, get_feature_file(file))\n",
    "    \n",
    "    df = pd.read_csv(labels_path, keep_default_na=False)\n",
    "    df.rename(\n",
    "        columns={\n",
    "            \"Unnamed: 0\": \"scene_id\",\n",
    "            \"0\": \"label\"\n",
    "        }, inplace=True\n",
    "    )\n",
    "    df[\"movie\"] = train_movies[idx]\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "train_labels = pd.concat(dfs, ignore_index=True)\n",
    "# print(train_labels.shape)\n",
    "# train_labels.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d9296-bf27-47ed-8a17-9aa1f7773a7d",
   "metadata": {},
   "source": [
    "### Merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "794727e1-b58f-4d9f-ba31-2aab8901196c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3729, 12)\n",
      "(2470, 11)\n"
     ]
    }
   ],
   "source": [
    "train_merged = merge_dataframes(train_timestamps, train_features)\n",
    "# additionaly merge labels to train data\n",
    "train_full_data = pd.merge(train_merged, train_labels, on=[\"scene_id\", \"movie\"], how=\"outer\")\n",
    "print(train_full_data.shape)\n",
    "\n",
    "test_full_data = merge_dataframes(test_timestamps, test_features)\n",
    "print(test_full_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c61e3-fdb8-45bd-9d9e-f47fff1e28fb",
   "metadata": {},
   "source": [
    "## Join Scenes with subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70dc66ea-b37c-47aa-87b7-94c0e4c754cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import srt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b2751-a43c-4cf1-9b6b-6ea8e573fc1e",
   "metadata": {},
   "source": [
    "### Helper runctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a04553f3-4e2f-4be1-9ea8-4202d4bc65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_movie_scenes_count = train_full_data.groupby('movie').count()['scene_id']\n",
    "# print(train_movie_scenes_count)\n",
    "test_movie_scenes_count = test_full_data.groupby('movie').count()['scene_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc802824-73fc-44cc-ac7d-855155e0b34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scene_id</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>movie</th>\n",
       "      <th>s_dur</th>\n",
       "      <th>n_shots</th>\n",
       "      <th>ava_shot_dur</th>\n",
       "      <th>rel_id_loc</th>\n",
       "      <th>rel_t_loc</th>\n",
       "      <th>ava_char_score</th>\n",
       "      <th>is_prot_appear</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>218.718333</td>\n",
       "      <td>the lost weekend</td>\n",
       "      <td>218.718333</td>\n",
       "      <td>6</td>\n",
       "      <td>36.453056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1295.579078</td>\n",
       "      <td>1</td>\n",
       "      <td>Opening Image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>218.760</td>\n",
       "      <td>240.740333</td>\n",
       "      <td>the lost weekend</td>\n",
       "      <td>21.980333</td>\n",
       "      <td>3</td>\n",
       "      <td>7.326778</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.036189</td>\n",
       "      <td>1295.579078</td>\n",
       "      <td>1</td>\n",
       "      <td>Opening Image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>240.782</td>\n",
       "      <td>282.574333</td>\n",
       "      <td>the lost weekend</td>\n",
       "      <td>41.792333</td>\n",
       "      <td>1</td>\n",
       "      <td>41.792333</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.039832</td>\n",
       "      <td>1256.344447</td>\n",
       "      <td>1</td>\n",
       "      <td>Set-Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>282.616</td>\n",
       "      <td>313.521333</td>\n",
       "      <td>the lost weekend</td>\n",
       "      <td>30.905333</td>\n",
       "      <td>1</td>\n",
       "      <td>30.905333</td>\n",
       "      <td>0.025210</td>\n",
       "      <td>0.046752</td>\n",
       "      <td>1256.344447</td>\n",
       "      <td>1</td>\n",
       "      <td>Set-Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>313.563</td>\n",
       "      <td>367.033333</td>\n",
       "      <td>the lost weekend</td>\n",
       "      <td>53.470333</td>\n",
       "      <td>4</td>\n",
       "      <td>13.367583</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.051871</td>\n",
       "      <td>1256.344447</td>\n",
       "      <td>1</td>\n",
       "      <td>Theme Stated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>220</td>\n",
       "      <td>6499.655</td>\n",
       "      <td>6574.354333</td>\n",
       "      <td>dallas buyers club</td>\n",
       "      <td>74.699333</td>\n",
       "      <td>16</td>\n",
       "      <td>4.668708</td>\n",
       "      <td>0.960699</td>\n",
       "      <td>0.926278</td>\n",
       "      <td>656.478716</td>\n",
       "      <td>1</td>\n",
       "      <td>Finale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>221</td>\n",
       "      <td>6574.396</td>\n",
       "      <td>6579.776333</td>\n",
       "      <td>dallas buyers club</td>\n",
       "      <td>5.380333</td>\n",
       "      <td>1</td>\n",
       "      <td>5.380333</td>\n",
       "      <td>0.965066</td>\n",
       "      <td>0.936929</td>\n",
       "      <td>372.108700</td>\n",
       "      <td>0</td>\n",
       "      <td>Finale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>224</td>\n",
       "      <td>6589.161</td>\n",
       "      <td>6623.987333</td>\n",
       "      <td>dallas buyers club</td>\n",
       "      <td>34.826333</td>\n",
       "      <td>6</td>\n",
       "      <td>5.804389</td>\n",
       "      <td>0.978166</td>\n",
       "      <td>0.939033</td>\n",
       "      <td>928.370357</td>\n",
       "      <td>1</td>\n",
       "      <td>Final Image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>226</td>\n",
       "      <td>6629.701</td>\n",
       "      <td>6682.462333</td>\n",
       "      <td>dallas buyers club</td>\n",
       "      <td>52.761333</td>\n",
       "      <td>19</td>\n",
       "      <td>2.776912</td>\n",
       "      <td>0.986900</td>\n",
       "      <td>0.944811</td>\n",
       "      <td>1076.267498</td>\n",
       "      <td>1</td>\n",
       "      <td>Final Image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>227</td>\n",
       "      <td>6682.504</td>\n",
       "      <td>6688.176333</td>\n",
       "      <td>dallas buyers club</td>\n",
       "      <td>5.672333</td>\n",
       "      <td>4</td>\n",
       "      <td>1.418083</td>\n",
       "      <td>0.991266</td>\n",
       "      <td>0.952336</td>\n",
       "      <td>2085.459362</td>\n",
       "      <td>1</td>\n",
       "      <td>Final Image</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3729 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      scene_id     start          end               movie       s_dur  \\\n",
       "0            0     0.000   218.718333    the lost weekend  218.718333   \n",
       "1            1   218.760   240.740333    the lost weekend   21.980333   \n",
       "2            2   240.782   282.574333    the lost weekend   41.792333   \n",
       "3            3   282.616   313.521333    the lost weekend   30.905333   \n",
       "4            4   313.563   367.033333    the lost weekend   53.470333   \n",
       "...        ...       ...          ...                 ...         ...   \n",
       "3724       220  6499.655  6574.354333  dallas buyers club   74.699333   \n",
       "3725       221  6574.396  6579.776333  dallas buyers club    5.380333   \n",
       "3726       224  6589.161  6623.987333  dallas buyers club   34.826333   \n",
       "3727       226  6629.701  6682.462333  dallas buyers club   52.761333   \n",
       "3728       227  6682.504  6688.176333  dallas buyers club    5.672333   \n",
       "\n",
       "      n_shots  ava_shot_dur  rel_id_loc  rel_t_loc  ava_char_score  \\\n",
       "0           6     36.453056    0.000000   0.000000     1295.579078   \n",
       "1           3      7.326778    0.008403   0.036189     1295.579078   \n",
       "2           1     41.792333    0.016807   0.039832     1256.344447   \n",
       "3           1     30.905333    0.025210   0.046752     1256.344447   \n",
       "4           4     13.367583    0.033613   0.051871     1256.344447   \n",
       "...       ...           ...         ...        ...             ...   \n",
       "3724       16      4.668708    0.960699   0.926278      656.478716   \n",
       "3725        1      5.380333    0.965066   0.936929      372.108700   \n",
       "3726        6      5.804389    0.978166   0.939033      928.370357   \n",
       "3727       19      2.776912    0.986900   0.944811     1076.267498   \n",
       "3728        4      1.418083    0.991266   0.952336     2085.459362   \n",
       "\n",
       "      is_prot_appear          label  \n",
       "0                  1  Opening Image  \n",
       "1                  1  Opening Image  \n",
       "2                  1         Set-Up  \n",
       "3                  1         Set-Up  \n",
       "4                  1   Theme Stated  \n",
       "...              ...            ...  \n",
       "3724               1         Finale  \n",
       "3725               0         Finale  \n",
       "3726               1    Final Image  \n",
       "3727               1    Final Image  \n",
       "3728               1    Final Image  \n",
       "\n",
       "[3729 rows x 12 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6768479e-06be-4fff-935f-7424174598ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_movies = [\"the ugly truth\", \"the social network\", \"the girl with the dragon tattoo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21b168cf-f6f1-429e-92ae-0f0f4e31033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_files_to_subtitles(files):\n",
    "    return [file.replace('.csv', '.srt') for file in files]\n",
    "\n",
    "def load_subtitles(paths, movies):\n",
    "    movie_subtitles = {}\n",
    "    for idx, movie_name in enumerate(movies):\n",
    "        # print(movie_name)\n",
    "        \n",
    "        if movie_name == \"pretty woman\":\n",
    "            with open(paths[idx], 'r', encoding='utf-16') as subtitle_file:\n",
    "                movie_subtitles[movie_name] = list(srt.parse(subtitle_file.read()))\n",
    "        elif movie_name in problematic_movies:\n",
    "            with open(paths[idx], 'r', encoding='utf-8', errors='replace') as subtitle_file:\n",
    "                movie_subtitles[movie_name] = list(srt.parse(subtitle_file.read()))\n",
    "        else:\n",
    "            with open(paths[idx], 'r', encoding='utf-8') as subtitle_file:\n",
    "                movie_subtitles[movie_name] = list(srt.parse(subtitle_file.read()))\n",
    "                \n",
    "    return movie_subtitles\n",
    "\n",
    "def associate_scenes_with_subtitles(full_data, movie_subtitles):\n",
    "    movies_scenes_subtitles = {}\n",
    "    additional_data = []\n",
    "    last_processed_subtitle_idx = 0\n",
    "    missed_subtitles = 0\n",
    "\n",
    "    for scene_idx, scene_row in full_data.iterrows():\n",
    "        scene_start, scene_end = scene_row['start'], scene_row['end']\n",
    "        scene_id, movie_name = scene_row['scene_id'], scene_row['movie']\n",
    "        \n",
    "        # Create new dictionary for every movie\n",
    "        if movie_name not in movies_scenes_subtitles:\n",
    "            movies_scenes_subtitles[movie_name] = {}\n",
    "            last_processed_subtitle_idx = 0\n",
    "\n",
    "        # Craete new dictionary for every scene within movie\n",
    "        movies_scenes_subtitles[movie_name][scene_id] = []\n",
    "\n",
    "        current_movie_subtitles = movie_subtitles[movie_name]\n",
    "        sentence_count = 0\n",
    "        for idx in range(last_processed_subtitle_idx, len(current_movie_subtitles)):\n",
    "            sub = current_movie_subtitles[idx]\n",
    "            sub_start, sub_end = sub.start.total_seconds(), sub.end.total_seconds()\n",
    "\n",
    "            # Some subtitles start just before scene_start\n",
    "            if scene_start <= (sub_start + 0.05) and scene_end >= sub_end:\n",
    "                # Add subtitle content to the dictionary for the current scene\n",
    "                movies_scenes_subtitles[movie_name][scene_id].append(sub.content)\n",
    "                sentence_count += sub.content.count('.') + sub.content.count('!') + sub.content.count('?')\n",
    "            elif scene_end < sub_end:\n",
    "                last_processed_subtitle_idx = idx\n",
    "                break\n",
    "            else:\n",
    "                missed_subtitles += 1\n",
    "        \n",
    "        additional_data.append({\n",
    "            'movie': movie_name,\n",
    "            'scene_id': scene_id,\n",
    "            'sentence_count': sentence_count,\n",
    "        })\n",
    "\n",
    "    print(f\"Ignored subtitltes: {missed_subtitles}\")\n",
    "    additional_df = pd.DataFrame(additional_data)\n",
    "    return movies_scenes_subtitles, additional_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8490b71-024e-451c-835c-a4408e2f39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(name, data):\n",
    "    formatted_data = \"{:.0f}\".format(data)\n",
    "    # print(f\"{name}{formatted_data}\")\n",
    "    return name + formatted_data\n",
    "\n",
    "\n",
    "def map_scene_location_to_category(x):\n",
    "    scene_mapping = {\n",
    "        (0, 1): \"Opening\",\n",
    "        (2, 15): \"Setup\",\n",
    "        (16, 19): \"Debate\",\n",
    "        (20, 49): \"Story\",\n",
    "        (50, 75): \"BadGuys\",\n",
    "        (76, 89): \"Ending\",\n",
    "        (90, 99): \"Finale\",\n",
    "        (99, float('inf')): \"FinalImage\"\n",
    "    }\n",
    "\n",
    "    for percentage_range, category in scene_mapping.items():\n",
    "        if percentage_range[0] <= x <= percentage_range[1]:\n",
    "            return category\n",
    "\n",
    "    return \"InvalidValue\"\n",
    "\n",
    "\n",
    "def associate_scenes_with_subtitles_extra_info(full_data, movie_subtitles, movie_scenes_count):\n",
    "    movies_scenes_subtitles = {}\n",
    "    last_processed_subtitle_idx = 0\n",
    "    missed_subtitles = 0\n",
    "\n",
    "    for scene_idx, scene_row in full_data.iterrows():\n",
    "        scene_start, scene_end = scene_row['start'], scene_row['end']\n",
    "        scene_id, movie_name = scene_row['scene_id'], scene_row['movie']\n",
    "        scene_location, time_location = scene_row['rel_id_loc'], scene_row['rel_t_loc'] \t\n",
    "        prot_appear = scene_row['is_prot_appear']\n",
    "        scene_count = movie_scenes_count[movie_name]\n",
    "\n",
    "        movie_name_without_spaces = movie_name.replace(\" \", \"\")\n",
    "        category = map_scene_location_to_category(int(scene_location * 100))\n",
    "        \n",
    "        scene_location_result = format_data(\"SceneLocation\", scene_location * 100)\n",
    "        time_location_result = format_data(\"TimeLocation\", time_location * 100)\n",
    "        category_result = f\"category{category}\"\n",
    "        movie_name_result = f\"MovieName{movie_name_without_spaces}\"\n",
    "        scene_start_result = format_data(\"SceneStart\", scene_start)\n",
    "        scene_end_result = format_data(\"SceneEnd\", scene_end)\n",
    "        prot_appear_result = format_data(\"ProtAppear\", prot_appear)\n",
    "        scene_count_result = format_data(\"SceneCount\", scene_count)\n",
    "        \n",
    "        # Create new dictionary for every movie\n",
    "        if movie_name not in movies_scenes_subtitles:\n",
    "            movies_scenes_subtitles[movie_name] = {}\n",
    "            last_processed_subtitle_idx = 0\n",
    "\n",
    "        # Craete new dictionary for every scene within movie\n",
    "        movies_scenes_subtitles[movie_name][scene_id] = []\n",
    "        movies_scenes_subtitles[movie_name][scene_id].append(scene_location_result)\n",
    "        movies_scenes_subtitles[movie_name][scene_id].append(time_location_result)\n",
    "        movies_scenes_subtitles[movie_name][scene_id].append(category_result)\n",
    "        movies_scenes_subtitles[movie_name][scene_id].append(movie_name_result)\n",
    "        movies_scenes_subtitles[movie_name][scene_id].append(scene_start_result)\n",
    "        movies_scenes_subtitles[movie_name][scene_id].append(scene_end_result)\n",
    "        movies_scenes_subtitles[movie_name][scene_id].append(prot_appear_result)\n",
    "        movies_scenes_subtitles[movie_name][scene_id].append(scene_count_result)\n",
    "\n",
    "        current_movie_subtitles = movie_subtitles[movie_name]\n",
    "        for idx in range(last_processed_subtitle_idx, len(current_movie_subtitles)):\n",
    "            sub = current_movie_subtitles[idx]\n",
    "            sub_start, sub_end = sub.start.total_seconds(), sub.end.total_seconds()\n",
    "\n",
    "            # Some subtitles start just before scene_start\n",
    "            if scene_start <= (sub_start + 0.05) and scene_end >= sub_end:\n",
    "                # Add subtitle content to the dictionary for the current scene\n",
    "                movies_scenes_subtitles[movie_name][scene_id].append(sub.content)\n",
    "            elif scene_end < sub_end:\n",
    "                last_processed_subtitle_idx = idx\n",
    "                break\n",
    "            else:\n",
    "                missed_subtitles += 1\n",
    "        \n",
    "    print(f\"Ignored subtitltes: {missed_subtitles}\")\n",
    "    return movies_scenes_subtitles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cb2a42-e9e0-4b51-8851-6f3c3989e397",
   "metadata": {},
   "source": [
    "### Get subtitle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fe0cf7b-cb14-4eb1-a65d-376ce385faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_files = load_files(FEATURES_PATH)\n",
    "# remove australia because australia subtitles are in diffrent format than everything else\n",
    "train_features_files.remove('tt0455824_australia.csv')\n",
    "test_features_files = load_files(TEST_FEATURES_PATH)\n",
    "\n",
    "train_subtitles_files = convert_files_to_subtitles(train_features_files)\n",
    "train_subtitle_paths = [os.path.join(SUBTITLES_PATH, subtitle_file) for subtitle_file in train_subtitles_files]\n",
    "\n",
    "test_subtitles_files = convert_files_to_subtitles(test_features_files)\n",
    "test_subtitle_paths = [os.path.join(TEST_SUBTITLES_PATH, subtitle_file) for subtitle_file in test_subtitles_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49999294-c505-4b00-a2be-0f8a5c24ff3e",
   "metadata": {},
   "source": [
    "### Load subtitles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c910f57-e1ec-4562-9f27-92e13cc52825",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_movie_subtitles = load_subtitles(train_subtitle_paths, train_movies)\n",
    "test_movie_subtitles = load_subtitles(test_subtitle_paths, test_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad00e5-ee25-4e9d-85de-44e39278862d",
   "metadata": {},
   "source": [
    "### Associate Scenes with subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c1dd419-8d7d-4c19-9d8d-74f5588d0413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored subtitltes: 3018\n",
      "Ignored subtitltes: 3165\n"
     ]
    }
   ],
   "source": [
    "train_movies_scenes_subtitles, additional_train_data = associate_scenes_with_subtitles(train_full_data, train_movie_subtitles)\n",
    "test_movies_scenes_subtitles, additional_test_data = associate_scenes_with_subtitles(test_full_data, test_movie_subtitles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c0a6fe",
   "metadata": {},
   "source": [
    "## Subtitles cleaning and combining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6281799-8ce1-4208-92ce-efe2058c4bd1",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbfc8d6b-af1f-4df9-a109-4813387446b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_subtitles(movies_scenes_subtitles):\n",
    "    for movie_name, scenes in movies_scenes_subtitles.items():\n",
    "        for scene_id, subtitles in scenes.items():\n",
    "            cleaned_subtitles = []\n",
    "            for subtitle in subtitles:\n",
    "                # Remove new line characters\n",
    "                cleaned_subtitle = subtitle.replace('\\n', ' ').strip()\n",
    "                cleaned_subtitle = re.sub(r'<.*?>', '', cleaned_subtitle)\n",
    "                cleaned_subtitle = re.sub(r'♪', '', cleaned_subtitle)\n",
    "                cleaned_subtitles.append(cleaned_subtitle)\n",
    "            \n",
    "            combined_text = \" \".join(cleaned_subtitles)\n",
    "            movies_scenes_subtitles[movie_name][scene_id] = combined_text\n",
    "            \n",
    "    return movies_scenes_subtitles\n",
    "\n",
    "def clean_subtitles_only_words(movies_scenes_subtitles):\n",
    "    for movie_name, scenes in movies_scenes_subtitles.items():\n",
    "        for scene_id, subtitles in scenes.items():\n",
    "            cleaned_subtitles = []\n",
    "            for subtitle in subtitles:\n",
    "                cleaned_subtitle = subtitle.replace('\\n', ' ').strip()\n",
    "                cleaned_subtitle = re.sub(r'<.*?>', '', cleaned_subtitle)\n",
    "                cleaned_subtitle = re.sub(r'[^a-zA-Z0-9\\s\\']', '', cleaned_subtitle)\n",
    "                cleaned_subtitles.append(cleaned_subtitle)\n",
    "            \n",
    "            combined_text = \" \".join(cleaned_subtitles)\n",
    "            movies_scenes_subtitles[movie_name][scene_id] = combined_text\n",
    "            \n",
    "    return movies_scenes_subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4d1034b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Although I can't dismiss   The memory of her kiss   I guess he's not for me \""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_movies_scenes_subtitles = clean_subtitles(train_movies_scenes_subtitles)\n",
    "clean_train_movies_scenes_subtitles['four weddings and a funeral'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48092566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"His majesty prefers not to be moistened. I got you a present. Oh. I hated this game. You loved it. You loved it. Thank you. I'll add it to the collection. Can you pour me a bourbon? (SIGHS) What's up, Jitters? (BREATHES DEEPLY) Well, if you're not going to talk... I'm gonna have to fill the silence... with another excruciating story by Margo Dunne. I could tell you about my recent customer service experience... changing Internet service providers. I like that one. Or how about the time... I saw that woman who looked exactly like my friend Monica? But it wasn't Monica. It was a total stranger. Who was also named Monica. Made it kind of interesting. It's great. I'm just having a bad day. Amy? It's our anniversary. Five years. Five? That came fast. And furious.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_movies_scenes_subtitles = clean_subtitles(test_movies_scenes_subtitles)\n",
    "clean_test_movies_scenes_subtitles['gone girl'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cafdd5",
   "metadata": {},
   "source": [
    "## Tokenize and Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963e0ad5-cd9a-4044-bfdf-237fd0e06f58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "847ea61b-7aaa-4e5e-9159-12e90fbe2141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\Studia\\Sem2-mgr\\ZED\\story-beats\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a44eb11b-52c0-47cf-bbdc-fef38ae1467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings(movies_scenes_subtitles):\n",
    "    embeddings = {}\n",
    "    for movie_name, scenes in movies_scenes_subtitles.items():\n",
    "        for scene_id, combined_text in scenes.items():\n",
    "            inputs = tokenizer(combined_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "            embeddings.setdefault(movie_name, {})[scene_id] = embedding\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8b776d9-c0ca-4228-8052-922d49878988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scene_id</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>movie</th>\n",
       "      <th>s_dur</th>\n",
       "      <th>n_shots</th>\n",
       "      <th>ava_shot_dur</th>\n",
       "      <th>rel_id_loc</th>\n",
       "      <th>rel_t_loc</th>\n",
       "      <th>ava_char_score</th>\n",
       "      <th>is_prot_appear</th>\n",
       "      <th>embedding</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>220</td>\n",
       "      <td>6499.655</td>\n",
       "      <td>6574.354333</td>\n",
       "      <td>dallas buyers club</td>\n",
       "      <td>74.699333</td>\n",
       "      <td>16</td>\n",
       "      <td>4.668708</td>\n",
       "      <td>0.960699</td>\n",
       "      <td>0.926278</td>\n",
       "      <td>656.478716</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.031814027577638626, 0.13735565543174744, 0....</td>\n",
       "      <td>Finale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>221</td>\n",
       "      <td>6574.396</td>\n",
       "      <td>6579.776333</td>\n",
       "      <td>dallas buyers club</td>\n",
       "      <td>5.380333</td>\n",
       "      <td>1</td>\n",
       "      <td>5.380333</td>\n",
       "      <td>0.965066</td>\n",
       "      <td>0.936929</td>\n",
       "      <td>372.108700</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.20158801972866058, 0.11704124510288239, 0.2...</td>\n",
       "      <td>Finale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>224</td>\n",
       "      <td>6589.161</td>\n",
       "      <td>6623.987333</td>\n",
       "      <td>dallas buyers club</td>\n",
       "      <td>34.826333</td>\n",
       "      <td>6</td>\n",
       "      <td>5.804389</td>\n",
       "      <td>0.978166</td>\n",
       "      <td>0.939033</td>\n",
       "      <td>928.370357</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.31051066517829895, 0.04721043258905411, -0....</td>\n",
       "      <td>Final Image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>226</td>\n",
       "      <td>6629.701</td>\n",
       "      <td>6682.462333</td>\n",
       "      <td>dallas buyers club</td>\n",
       "      <td>52.761333</td>\n",
       "      <td>19</td>\n",
       "      <td>2.776912</td>\n",
       "      <td>0.986900</td>\n",
       "      <td>0.944811</td>\n",
       "      <td>1076.267498</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.051616471260786057, -0.009972779080271721, ...</td>\n",
       "      <td>Final Image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>227</td>\n",
       "      <td>6682.504</td>\n",
       "      <td>6688.176333</td>\n",
       "      <td>dallas buyers club</td>\n",
       "      <td>5.672333</td>\n",
       "      <td>4</td>\n",
       "      <td>1.418083</td>\n",
       "      <td>0.991266</td>\n",
       "      <td>0.952336</td>\n",
       "      <td>2085.459362</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.3686937689781189, 0.022891951724886894, 0.2...</td>\n",
       "      <td>Final Image</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      scene_id     start          end               movie      s_dur  n_shots  \\\n",
       "3724       220  6499.655  6574.354333  dallas buyers club  74.699333       16   \n",
       "3725       221  6574.396  6579.776333  dallas buyers club   5.380333        1   \n",
       "3726       224  6589.161  6623.987333  dallas buyers club  34.826333        6   \n",
       "3727       226  6629.701  6682.462333  dallas buyers club  52.761333       19   \n",
       "3728       227  6682.504  6688.176333  dallas buyers club   5.672333        4   \n",
       "\n",
       "      ava_shot_dur  rel_id_loc  rel_t_loc  ava_char_score  is_prot_appear  \\\n",
       "3724      4.668708    0.960699   0.926278      656.478716               1   \n",
       "3725      5.380333    0.965066   0.936929      372.108700               0   \n",
       "3726      5.804389    0.978166   0.939033      928.370357               1   \n",
       "3727      2.776912    0.986900   0.944811     1076.267498               1   \n",
       "3728      1.418083    0.991266   0.952336     2085.459362               1   \n",
       "\n",
       "                                              embedding        label  \n",
       "3724  [0.031814027577638626, 0.13735565543174744, 0....       Finale  \n",
       "3725  [0.20158801972866058, 0.11704124510288239, 0.2...       Finale  \n",
       "3726  [0.31051066517829895, 0.04721043258905411, -0....  Final Image  \n",
       "3727  [0.051616471260786057, -0.009972779080271721, ...  Final Image  \n",
       "3728  [0.3686937689781189, 0.022891951724886894, 0.2...  Final Image  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_movies_scenes_embeddings = calculate_embeddings(clean_train_movies_scenes_subtitles)\n",
    "\n",
    "train_embeddings = []\n",
    "for index, row in train_full_data.iterrows():\n",
    "    movie_name, scene_id = row['movie'], row['scene_id']\n",
    "    embedding = train_movies_scenes_embeddings.get(movie_name, {}).get(scene_id, None)\n",
    "    train_embeddings.append(embedding)\n",
    "\n",
    "train_full_data.insert(train_full_data.columns.get_loc('label'), 'embedding', train_embeddings)\n",
    "train_full_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a282e8fe-0f92-4d39-96e9-a40ac5548f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scene_id</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>movie</th>\n",
       "      <th>s_dur</th>\n",
       "      <th>n_shots</th>\n",
       "      <th>ava_shot_dur</th>\n",
       "      <th>rel_id_loc</th>\n",
       "      <th>rel_t_loc</th>\n",
       "      <th>ava_char_score</th>\n",
       "      <th>is_prot_appear</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>216</td>\n",
       "      <td>8437.638</td>\n",
       "      <td>8460.952333</td>\n",
       "      <td>gone girl</td>\n",
       "      <td>23.314333</td>\n",
       "      <td>10</td>\n",
       "      <td>2.331433</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.943599</td>\n",
       "      <td>1400.736946</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.06389153748750687, 0.0513589084148407, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>217</td>\n",
       "      <td>8460.994</td>\n",
       "      <td>8543.284333</td>\n",
       "      <td>gone girl</td>\n",
       "      <td>82.290333</td>\n",
       "      <td>20</td>\n",
       "      <td>4.114517</td>\n",
       "      <td>0.977477</td>\n",
       "      <td>0.946211</td>\n",
       "      <td>1400.736946</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2014482319355011, 0.047157373279333115, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>218</td>\n",
       "      <td>8543.326</td>\n",
       "      <td>8565.640333</td>\n",
       "      <td>gone girl</td>\n",
       "      <td>22.314333</td>\n",
       "      <td>8</td>\n",
       "      <td>2.789292</td>\n",
       "      <td>0.981982</td>\n",
       "      <td>0.955418</td>\n",
       "      <td>1400.736946</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.38489431142807007, 0.030371684581041336, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>219</td>\n",
       "      <td>8565.682</td>\n",
       "      <td>8607.307333</td>\n",
       "      <td>gone girl</td>\n",
       "      <td>41.625333</td>\n",
       "      <td>9</td>\n",
       "      <td>4.625037</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.957918</td>\n",
       "      <td>1094.344390</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.07813607901334763, -0.12171444296836853, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>220</td>\n",
       "      <td>8607.349</td>\n",
       "      <td>8649.390333</td>\n",
       "      <td>gone girl</td>\n",
       "      <td>42.041333</td>\n",
       "      <td>6</td>\n",
       "      <td>7.006889</td>\n",
       "      <td>0.990991</td>\n",
       "      <td>0.962578</td>\n",
       "      <td>1029.305716</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.40521693229675293, 0.03550683706998825, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      scene_id     start          end      movie      s_dur  n_shots  \\\n",
       "2465       216  8437.638  8460.952333  gone girl  23.314333       10   \n",
       "2466       217  8460.994  8543.284333  gone girl  82.290333       20   \n",
       "2467       218  8543.326  8565.640333  gone girl  22.314333        8   \n",
       "2468       219  8565.682  8607.307333  gone girl  41.625333        9   \n",
       "2469       220  8607.349  8649.390333  gone girl  42.041333        6   \n",
       "\n",
       "      ava_shot_dur  rel_id_loc  rel_t_loc  ava_char_score  is_prot_appear  \\\n",
       "2465      2.331433    0.972973   0.943599     1400.736946               1   \n",
       "2466      4.114517    0.977477   0.946211     1400.736946               1   \n",
       "2467      2.789292    0.981982   0.955418     1400.736946               1   \n",
       "2468      4.625037    0.986486   0.957918     1094.344390               1   \n",
       "2469      7.006889    0.990991   0.962578     1029.305716               1   \n",
       "\n",
       "                                              embedding  \n",
       "2465  [-0.06389153748750687, 0.0513589084148407, 0.5...  \n",
       "2466  [0.2014482319355011, 0.047157373279333115, 0.2...  \n",
       "2467  [0.38489431142807007, 0.030371684581041336, 0....  \n",
       "2468  [0.07813607901334763, -0.12171444296836853, 0....  \n",
       "2469  [0.40521693229675293, 0.03550683706998825, 0.2...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_movies_scenes_embeddings = calculate_embeddings(test_movies_scenes_subtitles)\n",
    "\n",
    "test_embeddings = []\n",
    "for index, row in test_full_data.iterrows():\n",
    "    movie_name, scene_id = row['movie'], row['scene_id']\n",
    "    embedding = test_movies_scenes_embeddings.get(movie_name, {}).get(scene_id, None)\n",
    "    test_embeddings.append(embedding)\n",
    "\n",
    "test_full_data['embedding'] = test_embeddings\n",
    "test_full_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab7da3",
   "metadata": {},
   "source": [
    "## Add additionaly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9f9b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_data = pd.merge(train_full_data, additional_train_data, on=[\"movie\", \"scene_id\"])\n",
    "test_full_data = pd.merge(test_full_data, additional_test_data, on=[\"movie\", \"scene_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4334d013",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed7adb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# embeddings = np.array(train_full_data['embedding'].tolist())\n",
    "# pca = PCA(n_components=10)\n",
    "# reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "X = train_full_data[['scene_id', 'rel_id_loc', 'rel_t_loc', 'ava_char_score', 'ava_shot_dur', 'is_prot_appear']]\n",
    "movie_encoder = LabelEncoder()\n",
    "X['movie'] = movie_encoder.fit_transform(train_full_data['movie'])\n",
    "y = train_full_data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=23, test_size=0.3, stratify=y)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [210, 240, 200, 225, 250],\n",
    "    'min_samples_split': [15, 25, 35],\n",
    "    'min_samples_leaf': [10, 15, 20],\n",
    "    'max_depth': [10, 15, 20, 25, 30, 50]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(RandomForestClassifier(random_state=23), param_distributions=param_dist, n_iter=10, cv=5, random_state=23, n_jobs=-1)\n",
    "random_search.fit(X, y)\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e338f7",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = random_search.best_params_\n",
    "model = RandomForestClassifier(**best_params, random_state=23)\n",
    "model.fit(X, y)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "# scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "print(f\"Mean Accuracy from cross-validation on training set: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff470f4-0400-4ec3-a8bc-aac7c47e08be",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e086559-82e7-44a9-a159-970d9dbd8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, levels = pd.factorize(train_full_data['label'])\n",
    "print(classification_report(y,y_pred,target_names=levels))\n",
    "# print(classification_report(y_test,y_pred,target_names=levels))\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "print()\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81fc580-c101-41e6-b409-8fdf5003f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'model', 'X', and 'y' are already defined\n",
    "predicted = cross_val_predict(model, X, y, cv=5)\n",
    "conf_matrix = confusion_matrix(y, predicted)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eff19d-1730-4609-afd1-e5ddc12c0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "sns.heatmap(conf_matrix, fmt='g', ax=ax, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f97ce1",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "\n",
    "feature_names = [f\"feature {i}\" for i in range(X.shape[1])]\n",
    "feature_names = X.columns\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e1be6-db89-48ec-80d2-fa0a2a3c7cba",
   "metadata": {},
   "source": [
    "### Predict for real test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc314a-6a80-4fbd-b16c-f0a18825d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = np.array(test_full_data['embedding'].tolist())\n",
    "\n",
    "X = test_full_data[['scene_id', 'rel_id_loc', 'rel_t_loc', 'ava_char_score', 'ava_shot_dur', 'is_prot_appear']]\n",
    "movie_encoder = LabelEncoder()\n",
    "X['movie'] = movie_encoder.fit_transform(test_full_data['movie'])\n",
    "# X = np.hstack((X, embeddings))\n",
    "\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c0668-59ea-4db1-9ff7-7b0ba35f0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Shape of X:\", X.shape)\n",
    "# print(\"Shape of embeddings:\", embeddings.shape)\n",
    "print(y_pred.shape)\n",
    "print(test_full_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a274c22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee02df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "X = train_full_data[['s_dur','n_shots','ava_shot_dur','rel_id_loc','rel_t_loc','ava_char_score','is_prot_appear','sentence_count']]\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(train_full_data['label'])\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, random_state=23, test_size=0.3, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, random_state=23, test_size=0.5, stratify=y_temp)\n",
    "\n",
    "counts = Counter(y_train)\n",
    "undersample_strategy = {label: min(200, count) for label, count in counts.items()}\n",
    "\n",
    "oversample_strategy = {label: 200 for label in np.unique(y_train)}\n",
    "\n",
    "under = RandomUnderSampler(sampling_strategy=undersample_strategy, random_state=23)\n",
    "over = SMOTE(sampling_strategy=oversample_strategy, random_state=23)\n",
    "\n",
    "X_train_under, y_train_under = under.fit_resample(X_train, y_train)\n",
    "X_train_balanced, y_train_balanced = over.fit_resample(X_train_under, y_train_under)\n",
    "\n",
    "test_class_weights = compute_class_weight('balanced', classes=np.unique(y_train_balanced), y=y_train_balanced)\n",
    "test_class_weights_dict = dict(zip(np.unique(y_train_balanced), test_class_weights))\n",
    "test_sample_weight = np.array([test_class_weights_dict[class_label] for class_label in y_train_balanced])\n",
    "\n",
    "param_dist = {\n",
    "    'reg_alpha': [0, 0.001, 0.005, 0.01, 0.05],\n",
    "    'reg_lambda': [0.1, 0.5, 1.0, 1.5, 2.0],\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'max_depth': [5, 6, 7, 8, 9, 10],\n",
    "    'gamma': [0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'early_stopping_rounds': [2, 4, 6, 8, 10, 12, 14]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(random_state=23)\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=param_dist, n_iter=10, cv=5, random_state=23, n_jobs=-1)\n",
    "random_search.fit(X_train_balanced, y_train_balanced, eval_set=[(X_val, y_val)], verbose=True, sample_weight=test_sample_weight)\n",
    "\n",
    "print(\"Best parameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f445d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = random_search.best_params_\n",
    "model = XGBClassifier(**best_params, random_state=23)\n",
    "model.fit(X_train_balanced, y_train_balanced, eval_set=[(X_val, y_val)], verbose=True, sample_weight=test_sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37efd5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d71ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "# feature_names = [f\"feature {i}\" for i in range(X.shape[1])]\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(ax=ax)\n",
    "ax.set_title(\"Feature importances using XGBoost\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487bc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_full_data[['s_dur','n_shots','ava_shot_dur','rel_id_loc','rel_t_loc','ava_char_score','is_prot_appear','sentence_count']]\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "print(y_pred_labels.shape)\n",
    "print(test_full_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c67c4-3543-4bf9-959d-5383d5bcf6b1",
   "metadata": {},
   "source": [
    "## Text classification using BERT (Third Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b84f8-9543-4721-a87b-afc4ab827853",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72ac5d94-94cc-4be5-981a-2e50214397f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = train_full_data['label'].unique()\n",
    "label_to_int_mapping = {label: i for i, label in enumerate(unique_labels)}\n",
    "int_to_label_mapping = {i: label for label, i in label_to_int_mapping.items()}\n",
    "\n",
    "def map_label_to_int(label):\n",
    "    return label_to_int_mapping[label]\n",
    "\n",
    "\n",
    "def map_label_from_int(label):\n",
    "    return int_to_label_mapping[label] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0932200-b79c-4e09-9ea2-aef8836dcee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_subtitles(subtitles, df):\n",
    "    transformed_subtitles = []\n",
    "    for movie_name, scenes in subtitles.items():\n",
    "        for scene_id, text in scenes.items():\n",
    "            # Get the label from train_full_data based on movie and scene_id\n",
    "            label = df[(df['movie'] == movie_name) & (df['scene_id'] == scene_id)]['label'].values[0]\n",
    "            label = map_label_to_int(label)\n",
    "            \n",
    "            result = {'text': text, 'label': label}\n",
    "            transformed_subtitles.append(result)\n",
    "    \n",
    "    return transformed_subtitles\n",
    "\n",
    "\n",
    "def transform_validation_subtitles(subtitles, df):\n",
    "    transformed_subtitles = []\n",
    "    for movie_name, scenes in subtitles.items():\n",
    "        for scene_id, text in scenes.items():\n",
    "            result = {'text': text}\n",
    "            transformed_subtitles.append(result)\n",
    "    \n",
    "    return transformed_subtitles\n",
    "    \n",
    "\n",
    "def print_head_transformed_subtitles(subtiltes):\n",
    "    for i in range(min(5, len(subtiltes))):\n",
    "        print(subtiltes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe4be722-feb7-4bce-810b-022f90cac294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored subtitltes: 3018\n"
     ]
    }
   ],
   "source": [
    "train_subtitles = associate_scenes_with_subtitles_extra_info(train_full_data, train_movie_subtitles, train_movie_scenes_count)\n",
    "clean_train_subtitles = clean_subtitles_only_words(train_subtitles)\n",
    "train_transformed_subtitles = transform_subtitles(clean_train_subtitles, train_full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa321bf4-8951-4c93-9877-4f4171f18a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transformed_subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76971eb2-a85f-4571-a5b1-47e85ac57ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored subtitltes: 3165\n"
     ]
    }
   ],
   "source": [
    "validation_subtitles = associate_scenes_with_subtitles_extra_info(test_full_data, test_movie_subtitles, test_movie_scenes_count)\n",
    "clean_validation_subtitles = clean_subtitles_only_words(validation_subtitles)\n",
    "validation_transformed_subtitles = transform_validation_subtitles(clean_validation_subtitles, test_full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd8c9331-6143-467f-8705-73444fc9e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract 'text' and 'label' from each dictionary\n",
    "texts = [item['text'] for item in train_transformed_subtitles]\n",
    "labels = [item['label'] for item in train_transformed_subtitles]\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Create training and testing datasets\n",
    "train_data = [{'text': text, 'label': label} for text, label in zip(train_texts, train_labels)]\n",
    "test_data = [{'text': text, 'label': label} for text, label in zip(test_texts, test_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6011374-e926-47fd-bab2-bf9c7170ae84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 2610\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 1119\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2470\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "validation_dataset = Dataset.from_list(validation_transformed_subtitles)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "print(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a78ec02d-38fc-4256-8da8-b295f4011680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 482/482 [00:00<?, ?B/s] \n",
      "c:\\Users\\User\\Desktop\\Studia\\Sem2-mgr\\ZED\\story-beats\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 9.54MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.80MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 3.23MB/s]\n",
      "model.safetensors: 100%|██████████| 1.42G/1.42G [01:03<00:00, 22.4MB/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer\n",
    "\n",
    "def choose_model(name):\n",
    "    if name == \"distilbert\": \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=16)\n",
    "    elif name == \"bert\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=16)\n",
    "    elif name == \"roberta\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=16)\n",
    "    elif name == \"roberta-large\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large\", num_labels=16)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=16)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = choose_model(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e3e3f68-1fe4-4878-9454-fa39cc4bf3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236c0f2-4fee-44fb-a847-2cae1f6b902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3d9694f-4c62-4fec-be81-36499cacffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    examples[\"text\"] = [remove_stopwords(text) for text in examples[\"text\"]]\n",
    "    tokenized_text = tokenizer(examples[\"text\"], truncation=True)\n",
    "    \n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e215c3d-c652-4d9b-8656-b3ef83644c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2610 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2610/2610 [00:00<00:00, 7023.29 examples/s]\n",
      "Map: 100%|██████████| 1119/1119 [00:00<00:00, 16222.04 examples/s]\n",
      "Map: 100%|██████████| 2470/2470 [00:00<00:00, 15945.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "tokenized_validation_dataset = validation_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dbb940a6-e281-46c5-8c46-420f09ede81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1119\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31db1e78-cdac-467d-ad29-0de83423c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce2b0945-c1cd-4f70-89d0-c93ec711eb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e072f23a-1eb2-45a0-b872-a8efbda6caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=18,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5b4c5-d3e4-4a74-baec-cbc0da9f002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953c6a1f-dfd1-4fb6-afb4-8356414e67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe4e4ca-7999-4896-ac82-63f7ba6184a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = predictions.predictions\n",
    "predicted_label_ids = np.argmax(predicted_probabilities, axis=-1)\n",
    "\n",
    "label_predictions = [map_label_from_int(label) for label in predicted_label_ids]\n",
    "print(f\"first 10 predictions: {label_predictions[:10]} \\nlast 10 predictions: {label_predictions[-10:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49408d5e",
   "metadata": {},
   "source": [
    "## Result data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5406d67b-dde1-4782-82b4-af0e07c6302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(columns=['Id', 'Label'])\n",
    "\n",
    "def fill_result_df(full_data, movies, movies_ids, predictions):\n",
    "    for idx, scene in full_data.iterrows():\n",
    "        movie_name = scene['movie']\n",
    "        movie_index = movies.index(movie_name)\n",
    "        movie_id = movies_ids[movie_index]\n",
    "        scene_id = scene['scene_id']\n",
    "        movie_scene_id = f\"{movie_id}_{scene_id}\"\n",
    "\n",
    "        pred_label = predictions[idx]\n",
    "        result_df.loc[idx] = [movie_scene_id, pred_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f9776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First model (RF)\n",
    "# fill_result_df(test_full_data, test_movies, test_movies_ids, y_pred_labels)\n",
    "\n",
    "# Third model (Bert)\n",
    "fill_result_df(test_full_data, test_movies, test_movies_ids, label_predictions)\n",
    "\n",
    "\n",
    "result_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('output_roberta_18.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693669f-3972-4b60-9e3d-37efcef17888",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a9533-ceb5-40cb-8313-32123e578052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "folder_name = \"models\"\n",
    "\n",
    "def save_model(model, tokenizer, name):\n",
    "    model_folder_path = f\"./{folder_name}/{name}\" \n",
    "    model.save_model(model_folder_path)\n",
    "    tokenizer.save_pretrained(model_folder_path)\n",
    "    config = BertConfig.from_pretrained(model_folder_path)\n",
    "    config.save_pretrained(model_folder_path)\n",
    "    \n",
    "save_model(trainer, tokenizer, \"model_reoberta_18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc04cd-1d60-4f0a-b6fc-8457d7bfac7f",
   "metadata": {},
   "source": [
    "## Clean memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee0e77-7895-45be-9e93-6d42861d4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07906c6-d8af-45bb-ad19-95b3f2bb02c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44d598-d978-4195-a66c-a4d749c3cfa5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Save combined clean data in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0ad960-5f5e-4710-8361-0c38f8424d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"output_combined_file.txt\"  # Set your desired output file\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    for movie, scenes in clean_train_movies_scenes_subtitles.items():\n",
    "        output_file.write(f\"======= {movie} =========\\n\\n\")\n",
    "        output_file.write(str(scenes))\n",
    "\n",
    "print(\"Combined text file saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52291fce-b96a-4304-9ab7-8a85c230ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"output_combined_file2.txt\"  # Set your desired output file\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    for movie, scenes in clean_train_subtitles.items():\n",
    "        output_file.write(f\"======= {movie} =========\\n\\n\")\n",
    "        output_file.write(str(scenes))\n",
    "\n",
    "print(\"Combined text file saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0c8da",
   "metadata": {},
   "source": [
    "## Models Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0bf89a13-37d5-4024-bb7f-d456a740bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def merge_csv_files(folder_path, output_file, file_accuracies):\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    id_labels = {}\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(folder_path, csv_file)\n",
    "        current_df = pd.read_csv(file_path, keep_default_na=False)\n",
    "\n",
    "        weight = file_accuracies.get(csv_file, 1) if file_accuracies else 1\n",
    "\n",
    "        for _, row in current_df.iterrows():\n",
    "            current_id = row['Id']\n",
    "            current_label = row['Label']\n",
    "\n",
    "            if current_id not in id_labels:\n",
    "                id_labels[current_id] = {'labels': Counter()}\n",
    "\n",
    "            id_labels[current_id]['labels'][current_label] += 1 * weight\n",
    "\n",
    "    merged_data = {'Id': [], 'Label': []}\n",
    "    for current_id, labels in id_labels.items():\n",
    "        if current_id == 'tt0822832_84':\n",
    "            print(current_id, labels)\n",
    "        most_common_label = get_most_common_label(labels)\n",
    "\n",
    "        merged_data['Id'].append(current_id)\n",
    "        merged_data['Label'].append(most_common_label)\n",
    "\n",
    "    merged_df = pd.DataFrame(merged_data)\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "def get_most_common_label(data):\n",
    "    label_counts = data['labels']\n",
    "    most_common_label = max(label_counts, key=label_counts.get)\n",
    "    return most_common_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f8be0dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tt0822832_84 {'labels': Counter({'None': 1.9009, 'Dark Night of the Soul': 1.84436, 'All Is Lost': 0.45555})}\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'models_voting/'\n",
    "output_file = 'output_voting.csv'\n",
    "\n",
    "file_accuracies = {'output_bert_5.csv': 0.47035, 'output_bert_10.csv': 0.47503, 'output_bert_15.csv': 0.47777, 'output_bert_30.csv': 0.44383,\n",
    "                   'output_roberta_12.csv': 0.47035, 'output_roberta_15.csv': 0.48517, 'output_roberta_50.csv': 0.45555, 'output_random_forest.csv': 0.48439,\n",
    "                   'output_roberta_large_12.csv': 0.43837}\n",
    "\n",
    "merge_csv_files(folder_path, output_file, file_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
