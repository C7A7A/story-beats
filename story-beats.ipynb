{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4750b17-3e17-4bea-ac92-0886ca139561",
   "metadata": {},
   "source": [
    "## Imports and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1639528a-1137-4270-8c2b-b3ef481c427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4526c69-03c4-4a09-a494-a3f00d9c2fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMPS_PATH = 'train/train/scene_timestamps'\n",
    "FEATURES_PATH = 'train/train/features'\n",
    "LABELS_PATH = 'train/train/labels'\n",
    "SUBTITLES_PATH = 'train/train/subtitles'\n",
    "\n",
    "TEST_TIMESTAMPS_PATH = 'test/test/scene_timestamps'\n",
    "TEST_FEATURES_PATH = 'test/test/features'\n",
    "TEST_SUBTITLES_PATH = 'test/test/subtitles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a14a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check GPU available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000a3d5-7031-41ca-8e30-21753aacfbb6",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2e1c0-23af-428c-8112-10f7e0d6806b",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac399c52-72f6-4643-be29-7f922e00447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(path):\n",
    "    if os.path.exists(path):\n",
    "        return os.listdir(path)\n",
    "    else:\n",
    "        print(\"PATH DOES NOT EXIST!\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccb21fde-14d1-48e9-843d-adec49a4c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_name(file):\n",
    "    return file[10:-15]\n",
    "\n",
    "def get_movie_id(file):\n",
    "    return file[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "536fd822-2c92-493b-b3d6-b13fa53a725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_file(file):\n",
    "    return file.replace(\"_timestamps\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6315a7a-d497-4fd4-b13f-0d221a94634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_csv(path, movie_name):\n",
    "    df = pd.read_csv(path)\n",
    "    df.rename(columns={\"Unnamed: 0\": \"scene_id\"}, inplace=True)\n",
    "    df[\"movie\"] = movie_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "487bbef4-02d3-4e56-b7df-d2abacfb907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_movie_info(files):\n",
    "    movie_ids, movies = [], []\n",
    "    for file in files:\n",
    "        movie_name = get_movie_name(file)\n",
    "        movie_id = get_movie_id(file)\n",
    "        movie_ids.append(movie_id)\n",
    "        movies.append(movie_name)\n",
    "    return movie_ids, movies\n",
    "\n",
    "def prepare_dataframes(files, movies, path_func, data_func):\n",
    "    dfs = []\n",
    "    for idx, file in enumerate(files):\n",
    "        path = path_func(file)\n",
    "        df = data_func(path, movies[idx])\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def merge_dataframes(df1, df2):\n",
    "    test_full_data = pd.merge(df1, df2, on=[\"scene_id\", \"movie\"])\n",
    "    test_full_data[\"end\"] = test_full_data[\"start\"] + test_full_data[\"s_dur\"]\n",
    "    return test_full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859a0bf-eda8-445d-8c8a-4d166393b1c9",
   "metadata": {},
   "source": [
    "### Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a2fabdc-1f16-4dfb-9b0d-e761b72e231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = load_files(TIMESTAMPS_PATH)\n",
    "# remove australia because australia subtitles are in diffrent format than everything else\n",
    "train_files.remove(\"tt0455824_australia_timestamps.csv\")\n",
    "\n",
    "test_files = load_files(TEST_TIMESTAMPS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd91cd-a851-4157-9860-850ad3737162",
   "metadata": {},
   "source": [
    "### Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6798481c-e78e-4f88-87ea-4a0fa6451f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_movies_ids, train_movies = extract_movie_info(train_files)\n",
    "test_movies_ids, test_movies = extract_movie_info(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f59cca-5f69-44f2-9246-539cce0e99ae",
   "metadata": {},
   "source": [
    "### Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b51c99ee-7ffc-4df7-9c7c-932e7343bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_timestamps = prepare_dataframes(\n",
    "    train_files,\n",
    "    train_movies,\n",
    "    lambda file: os.path.join(TIMESTAMPS_PATH, file), \n",
    "    prepare_csv\n",
    ")\n",
    "\n",
    "test_timestamps = prepare_dataframes(\n",
    "    test_files,\n",
    "    test_movies,\n",
    "    lambda file: os.path.join(TEST_TIMESTAMPS_PATH, file), \n",
    "    prepare_csv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2befcf-73ce-4bde-8e7f-febf7891d147",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc254cd7-894b-4afa-9f26-c31b3974db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = prepare_dataframes(\n",
    "    train_files,\n",
    "    train_movies,\n",
    "    lambda file: os.path.join(FEATURES_PATH, get_feature_file(file)), \n",
    "    prepare_csv\n",
    ")\n",
    "\n",
    "test_features = prepare_dataframes(\n",
    "    test_files,\n",
    "    test_movies,\n",
    "    lambda file: os.path.join(TEST_FEATURES_PATH, get_feature_file(file)), \n",
    "    prepare_csv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8467b2d-3432-42a9-964c-226ac931c302",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b36a21f1-9d62-4a9f-a51f-c355a49a96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for idx, file in enumerate(train_files):\n",
    "    # print(feature_files[idx])\n",
    "    labels_path = os.path.join(LABELS_PATH, get_feature_file(file))\n",
    "    \n",
    "    df = pd.read_csv(labels_path, keep_default_na=False)\n",
    "    df.rename(\n",
    "        columns={\n",
    "            \"Unnamed: 0\": \"scene_id\",\n",
    "            \"0\": \"label\"\n",
    "        }, inplace=True\n",
    "    )\n",
    "    df[\"movie\"] = train_movies[idx]\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "train_labels = pd.concat(dfs, ignore_index=True)\n",
    "# print(train_labels.shape)\n",
    "# train_labels.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d9296-bf27-47ed-8a17-9aa1f7773a7d",
   "metadata": {},
   "source": [
    "### Merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "794727e1-b58f-4d9f-ba31-2aab8901196c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3729, 12)\n",
      "(2470, 11)\n"
     ]
    }
   ],
   "source": [
    "train_merged = merge_dataframes(train_timestamps, train_features)\n",
    "# additionaly merge labels to train data\n",
    "train_full_data = pd.merge(train_merged, train_labels, on=[\"scene_id\", \"movie\"], how=\"outer\")\n",
    "print(train_full_data.shape)\n",
    "\n",
    "test_full_data = merge_dataframes(test_timestamps, test_features)\n",
    "print(test_full_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c61e3-fdb8-45bd-9d9e-f47fff1e28fb",
   "metadata": {},
   "source": [
    "## Join Scenes with subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70dc66ea-b37c-47aa-87b7-94c0e4c754cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import srt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b2751-a43c-4cf1-9b6b-6ea8e573fc1e",
   "metadata": {},
   "source": [
    "### Helper runctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6768479e-06be-4fff-935f-7424174598ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_movies = [\"the ugly truth\", \"the social network\", \"the girl with the dragon tattoo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21b168cf-f6f1-429e-92ae-0f0f4e31033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_files_to_subtitles(files):\n",
    "    return [file.replace('.csv', '.srt') for file in files]\n",
    "\n",
    "def load_subtitles(paths, movies):\n",
    "    movie_subtitles = {}\n",
    "    for idx, movie_name in enumerate(movies):\n",
    "        # print(movie_name)\n",
    "        \n",
    "        if movie_name == \"pretty woman\":\n",
    "            with open(paths[idx], 'r', encoding='utf-16') as subtitle_file:\n",
    "                movie_subtitles[movie_name] = list(srt.parse(subtitle_file.read()))\n",
    "        elif movie_name in problematic_movies:\n",
    "            with open(paths[idx], 'r', encoding='utf-8', errors='replace') as subtitle_file:\n",
    "                movie_subtitles[movie_name] = list(srt.parse(subtitle_file.read()))\n",
    "        else:\n",
    "            with open(paths[idx], 'r', encoding='utf-8') as subtitle_file:\n",
    "                movie_subtitles[movie_name] = list(srt.parse(subtitle_file.read()))\n",
    "                \n",
    "    return movie_subtitles\n",
    "\n",
    "def associate_scenes_with_subtitles(full_data, movie_subtitles):\n",
    "    movies_scenes_subtitles = {}\n",
    "    additional_data = []\n",
    "    last_processed_subtitle_idx = 0\n",
    "    missed_subtitles = 0\n",
    "\n",
    "    for scene_idx, scene_row in full_data.iterrows():\n",
    "        scene_start, scene_end = scene_row['start'], scene_row['end']\n",
    "        scene_id, movie_name = scene_row['scene_id'], scene_row['movie']\n",
    "        \n",
    "        # Create new dictionary for every movie\n",
    "        if movie_name not in movies_scenes_subtitles:\n",
    "            movies_scenes_subtitles[movie_name] = {}\n",
    "            last_processed_subtitle_idx = 0\n",
    "\n",
    "        # Craete new dictionary for every scene within movie\n",
    "        movies_scenes_subtitles[movie_name][scene_id] = []\n",
    "\n",
    "        current_movie_subtitles = movie_subtitles[movie_name]\n",
    "        sentence_count = 0\n",
    "        dialog_detected = False\n",
    "        for idx in range(last_processed_subtitle_idx, len(current_movie_subtitles)):\n",
    "            sub = current_movie_subtitles[idx]\n",
    "            sub_start, sub_end = sub.start.total_seconds(), sub.end.total_seconds()\n",
    "\n",
    "            # Some subtitles start just before scene_start\n",
    "            if scene_start <= (sub_start + 0.05) and scene_end >= sub_end:\n",
    "                # Add subtitle content to the dictionary for the current scene\n",
    "                movies_scenes_subtitles[movie_name][scene_id].append(sub.content)\n",
    "                sentence_count += sub.content.count('.') + sub.content.count('!') + sub.content.count('?')\n",
    "                if sub.content.strip().startswith('-'):\n",
    "                    dialog_detected = True\n",
    "            elif scene_end < sub_end:\n",
    "                last_processed_subtitle_idx = idx\n",
    "                break\n",
    "            else:\n",
    "                missed_subtitles += 1\n",
    "        \n",
    "        additional_data.append({\n",
    "            'movie_name': movie_name,\n",
    "            'scene_id': scene_id,\n",
    "            'sentence_count': sentence_count,\n",
    "            'dialog_detected ': 1 if dialog_detected  else 0\n",
    "        })\n",
    "\n",
    "    print(f\"Ignored subtitltes: {missed_subtitles}\")\n",
    "    additional_df = pd.DataFrame(additional_data)\n",
    "    return movies_scenes_subtitles, additional_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cb2a42-e9e0-4b51-8851-6f3c3989e397",
   "metadata": {},
   "source": [
    "### Get subtitle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fe0cf7b-cb14-4eb1-a65d-376ce385faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_files = load_files(FEATURES_PATH)\n",
    "# remove australia because australia subtitles are in diffrent format than everything else\n",
    "train_features_files.remove('tt0455824_australia.csv')\n",
    "test_features_files = load_files(TEST_FEATURES_PATH)\n",
    "\n",
    "train_subtitles_files = convert_files_to_subtitles(train_features_files)\n",
    "train_subtitle_paths = [os.path.join(SUBTITLES_PATH, subtitle_file) for subtitle_file in train_subtitles_files]\n",
    "\n",
    "test_subtitles_files = convert_files_to_subtitles(test_features_files)\n",
    "test_subtitle_paths = [os.path.join(TEST_SUBTITLES_PATH, subtitle_file) for subtitle_file in test_subtitles_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49999294-c505-4b00-a2be-0f8a5c24ff3e",
   "metadata": {},
   "source": [
    "### Load subtitles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c910f57-e1ec-4562-9f27-92e13cc52825",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_movie_subtitles = load_subtitles(train_subtitle_paths, train_movies)\n",
    "test_movie_subtitles = load_subtitles(test_subtitle_paths, test_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad00e5-ee25-4e9d-85de-44e39278862d",
   "metadata": {},
   "source": [
    "### Associate Scenes with subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c1dd419-8d7d-4c19-9d8d-74f5588d0413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored subtitltes: 3018\n",
      "Ignored subtitltes: 3165\n"
     ]
    }
   ],
   "source": [
    "train_movies_scenes_subtitles, additional_train_data = associate_scenes_with_subtitles(train_full_data, train_movie_subtitles)\n",
    "test_movies_scenes_subtitles, additional_test_data = associate_scenes_with_subtitles(test_full_data, test_movie_subtitles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c0a6fe",
   "metadata": {},
   "source": [
    "## Subtitles cleaning and combining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6281799-8ce1-4208-92ce-efe2058c4bd1",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbfc8d6b-af1f-4df9-a109-4813387446b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_subtitles(movies_scenes_subtitles):\n",
    "    for movie_name, scenes in movies_scenes_subtitles.items():\n",
    "        for scene_id, subtitles in scenes.items():\n",
    "            cleaned_subtitles = []\n",
    "            for subtitle in subtitles:\n",
    "                # Remove new line characters\n",
    "                cleaned_subtitle = subtitle.replace('\\n', ' ').strip()\n",
    "                cleaned_subtitle = re.sub(r'<.*?>', '', cleaned_subtitle)\n",
    "                cleaned_subtitle = re.sub(r'♪', '', cleaned_subtitle)\n",
    "                cleaned_subtitles.append(cleaned_subtitle)\n",
    "            \n",
    "            combined_text = \" \".join(cleaned_subtitles)\n",
    "            movies_scenes_subtitles[movie_name][scene_id] = combined_text\n",
    "            \n",
    "    return movies_scenes_subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4d1034b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Although I can't dismiss   The memory of her kiss   I guess he's not for me \""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_movies_scenes_subtitles = clean_subtitles(train_movies_scenes_subtitles)\n",
    "clean_train_movies_scenes_subtitles['four weddings and a funeral'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48092566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"His majesty prefers not to be moistened. I got you a present. Oh. I hated this game. You loved it. You loved it. Thank you. I'll add it to the collection. Can you pour me a bourbon? (SIGHS) What's up, Jitters? (BREATHES DEEPLY) Well, if you're not going to talk... I'm gonna have to fill the silence... with another excruciating story by Margo Dunne. I could tell you about my recent customer service experience... changing Internet service providers. I like that one. Or how about the time... I saw that woman who looked exactly like my friend Monica? But it wasn't Monica. It was a total stranger. Who was also named Monica. Made it kind of interesting. It's great. I'm just having a bad day. Amy? It's our anniversary. Five years. Five? That came fast. And furious.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_movies_scenes_subtitles = clean_subtitles(test_movies_scenes_subtitles)\n",
    "clean_test_movies_scenes_subtitles['gone girl'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cafdd5",
   "metadata": {},
   "source": [
    "## Tokenize and Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963e0ad5-cd9a-4044-bfdf-237fd0e06f58",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ea61b-7aaa-4e5e-9159-12e90fbe2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44eb11b-52c0-47cf-bbdc-fef38ae1467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings(movies_scenes_subtitles):\n",
    "    embeddings = {}\n",
    "    for movie_name, scenes in movies_scenes_subtitles.items():\n",
    "        for scene_id, combined_text in scenes.items():\n",
    "            inputs = tokenizer(combined_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "            embeddings.setdefault(movie_name, {})[scene_id] = embedding\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b776d9-c0ca-4228-8052-922d49878988",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_movies_scenes_embeddings = calculate_embeddings(clean_train_movies_scenes_subtitles)\n",
    "\n",
    "train_embeddings = []\n",
    "for index, row in train_full_data.iterrows():\n",
    "    movie_name, scene_id = row['movie'], row['scene_id']\n",
    "    embedding = train_movies_scenes_embeddings.get(movie_name, {}).get(scene_id, None)\n",
    "    train_embeddings.append(embedding)\n",
    "\n",
    "train_full_data.insert(train_full_data.columns.get_loc('label'), 'embedding', train_embeddings)\n",
    "train_full_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282e8fe-0f92-4d39-96e9-a40ac5548f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_movies_scenes_embeddings = calculate_embeddings(test_movies_scenes_subtitles)\n",
    "\n",
    "test_embeddings = []\n",
    "for index, row in test_full_data.iterrows():\n",
    "    movie_name, scene_id = row['movie'], row['scene_id']\n",
    "    embedding = test_movies_scenes_embeddings.get(movie_name, {}).get(scene_id, None)\n",
    "    test_embeddings.append(embedding)\n",
    "\n",
    "test_full_data['embedding'] = test_embeddings\n",
    "test_full_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4334d013",
   "metadata": {},
   "source": [
    "## First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed7adb",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f292ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "embeddings = np.array(train_full_data['embedding'].tolist())\n",
    "pca = PCA(n_components=10)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# X = train_full_data[['s_dur', 'n_shots', 'ava_shot_dur', 'rel_id_loc', 'rel_t_loc', 'ava_char_score', 'is_prot_appear']]\n",
    "X = train_full_data[['rel_id_loc', 'rel_t_loc', 'ava_char_score']]\n",
    "# X = np.hstack((X, reduced_embeddings))\n",
    "y = train_full_data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=23, test_size=0.3, stratify=y)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [210, 240, 200, 225, 250],\n",
    "    'min_samples_split': [15, 25, 35],\n",
    "    'min_samples_leaf': [10, 15, 20],\n",
    "    'max_depth': [10, 15, 20, 25, 30, 50]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(RandomForestClassifier(random_state=23), param_distributions=param_dist, n_iter=10, cv=5, random_state=23, n_jobs=-1)\n",
    "# random_search.fit(X, y)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e338f7",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RandomForestClassifier(random_state=23, n_estimators=210, min_samples_split=25, min_samples_leaf=15, max_depth=25)\n",
    "model = RandomForestClassifier(random_state=23, n_estimators=225, min_samples_split=35, min_samples_leaf=15, max_depth=25)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "print(f\"Mean Accuracy from cross-validation on training set: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff470f4-0400-4ec3-a8bc-aac7c47e08be",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e086559-82e7-44a9-a159-970d9dbd8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, levels = pd.factorize(train_full_data['label'])\n",
    "print(classification_report(y_test,y_pred,target_names=levels))\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print()\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81fc580-c101-41e6-b409-8fdf5003f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'model', 'X', and 'y' are already defined\n",
    "predicted = cross_val_predict(model, X, y, cv=5)\n",
    "conf_matrix = confusion_matrix(y, predicted)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eff19d-1730-4609-afd1-e5ddc12c0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "sns.heatmap(conf_matrix, fmt='g', ax=ax, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f97ce1",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "\n",
    "feature_names = [f\"feature {i}\" for i in range(X.shape[1])]\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e1be6-db89-48ec-80d2-fa0a2a3c7cba",
   "metadata": {},
   "source": [
    "### Predict for real test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc314a-6a80-4fbd-b16c-f0a18825d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(test_full_data['embedding'].tolist())\n",
    "\n",
    "#X = test_full_data[['s_dur', 'n_shots', 'ava_shot_dur', 'rel_id_loc', 'rel_t_loc', 'ava_char_score', 'is_prot_appear']]\n",
    "X = test_full_data[['rel_id_loc', 'rel_t_loc', 'ava_char_score']]\n",
    "# X = np.hstack((X, embeddings))\n",
    "\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c0668-59ea-4db1-9ff7-7b0ba35f0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Shape of X:\", X.shape)\n",
    "# print(\"Shape of embeddings:\", embeddings.shape)\n",
    "print(y_pred.shape)\n",
    "print(test_full_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a274c22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b280aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d94b798",
   "metadata": {},
   "source": [
    "### LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4934167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = []\n",
    "for movie, scenes in movies_scenes_subtitles.items():\n",
    "    for scene_number, text in scenes.items():\n",
    "        data.append({\"movie\": movie, \"scene\": scene_number, \"text\": text})\n",
    "scenes_df = pd.DataFrame(data)\n",
    "\n",
    "full_data.insert(full_data.columns.get_loc('label'), 'text', scenes_df['text'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "full_data['encoded_label'] = label_encoder.fit_transform(full_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036b0dc",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9229cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = full_data.drop(columns=['movie', 'encoded_label'])\n",
    "y = full_data['encoded_label']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, random_state=23, test_size=0.3, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, random_state=23, test_size=0.5, stratify=y_temp)\n",
    "\n",
    "train_texts = X_train['text']\n",
    "val_texts = X_val['text']\n",
    "test_texts = X_test['text']\n",
    "\n",
    "train_labels = y_train\n",
    "val_labels = y_val\n",
    "test_labels = y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe9d03f",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def encode_texts(texts, labels):\n",
    "    encodings = tokenizer(texts.tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    labels = torch.tensor(labels)\n",
    "    return encodings, labels\n",
    "\n",
    "train_encodings, train_labels = encode_texts(train_texts, train_labels.values)\n",
    "val_encodings, val_labels = encode_texts(val_texts, val_labels.values)\n",
    "test_encodings, test_labels = encode_texts(test_texts, test_labels.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e5fbe",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ffa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_encodings['input_ids'], \n",
    "                              train_encodings['attention_mask'], \n",
    "                              train_labels)\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], \n",
    "                            val_encodings['attention_mask'], \n",
    "                            val_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], \n",
    "                             test_encodings['attention_mask'], \n",
    "                             test_labels)\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b89e0",
   "metadata": {},
   "source": [
    "### Trening and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(label_encoder.classes_))\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "def calculate_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Trening Model\n",
    "epochs = 12\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        b_labels = b_labels.long()\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Loss: {avg_train_loss}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "\n",
    "    for batch in val_loader:\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += calculate_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_loader)\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Validation Accuracy: {avg_val_accuracy}')\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc500a76",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6672e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_test_accuracy = 0\n",
    "\n",
    "for batch in test_loader:\n",
    "    b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    total_test_accuracy += calculate_accuracy(logits, label_ids)\n",
    "\n",
    "avg_test_accuracy = total_test_accuracy / len(test_loader)\n",
    "print(f'Test Accuracy: {avg_test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49408d5e",
   "metadata": {},
   "source": [
    "## Result data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f9776",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(columns=['Id', 'Label'])\n",
    "\n",
    "for idx, scene in test_full_data.iterrows():\n",
    "    movie_name = scene['movie']\n",
    "    movie_index = test_movies.index(movie_name)\n",
    "    movie_id = test_movies_ids[movie_index]\n",
    "    scene_id = scene['scene_id']\n",
    "    movie_scene_id = f\"{movie_id}_{scene_id}\"\n",
    "\n",
    "    pred_label = y_pred[idx]\n",
    "\n",
    "    result_df.loc[idx] = [movie_scene_id, pred_label]\n",
    "\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
